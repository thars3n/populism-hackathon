{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and set up logging\n",
    "import gensim \n",
    "import logging\n",
    "import glob, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing all source texts for training the model \n",
    "data_dir=\"/Users/tharsen/Desktop/populism-hackathon/corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1H4_h.txt = 130423 chars\n",
      "1H6_h.txt = 116317 chars\n",
      "2H4_h.txt = 141574 chars\n",
      "2H6_h.txt = 135863 chars\n",
      "3H6_h.txt = 129551 chars\n",
      "Ado_c.txt = 111116 chars\n",
      "Ant_t.txt = 133668 chars\n",
      "AWW_c.txt = 121896 chars\n",
      "AYL_c.txt = 114429 chars\n",
      "Cor_t.txt = 146443 chars\n",
      "Cym_t.txt = 147159 chars\n",
      "Err_c.txt = 76924 chars\n",
      "H5_h.txt = 141819 chars\n",
      "H8_h.txt = 128223 chars\n",
      "Ham_t.txt = 163429 chars\n",
      "JC_t.txt = 104561 chars\n",
      "John_t.txt = 112414 chars\n",
      "Lear_t.txt = 140510 chars\n",
      "LLL_c.txt = 115391 chars\n",
      "Lucrece_x.txt = 86177 chars\n",
      "M4M_c.txt = 116348 chars\n",
      "Mac_t.txt = 91625 chars\n",
      "MerchV_c.txt = 112334 chars\n",
      "MND_c.txt = 88608 chars\n",
      "Oth_t.txt = 141395 chars\n",
      "Pericles_x.txt = 97471 chars\n",
      "PhxTur_x.txt = 2072 chars\n",
      "R2_h.txt = 120934 chars\n",
      "R3_h.txt = 156881 chars\n",
      "Rom_t.txt = 130885 chars\n",
      "Shr_c.txt = 111364 chars\n",
      "Sonnets_x.txt = 97204 chars\n",
      "TGV_c.txt = 91686 chars\n",
      "Tim_t.txt = 98749 chars\n",
      "Tit_t.txt = 109892 chars\n",
      "Tmp_c.txt = 88800 chars\n",
      "TN_c.txt = 104476 chars\n",
      "TNK_x.txt = 127691 chars\n",
      "Tro_c.txt = 142635 chars\n",
      "VenusAdonis_x.txt = 55527 chars\n",
      "Wiv_c.txt = 115202 chars\n",
      "WT_c.txt = 134528 chars\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_dir)\n",
    "documents = list()\n",
    "for filename in glob.glob(\"*.txt\"):\n",
    "    filedata = open(filename, 'r').read()\n",
    "    print(filename + \" = \" + str(len(filedata)) + \" chars\")\n",
    "    documents = documents + filedata.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " So shaken as we are , so wan with care , Find we a time for frighted peace to pant And breathe short-winded accents of new broils To be commenced in strands afar remote \n"
     ]
    }
   ],
   "source": [
    "# Check to see that the first sentence is correct\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-11 14:47:41,114 : INFO : collecting all words and their counts\n",
      "2019-02-11 14:47:41,115 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2019-02-11 14:47:41,483 : INFO : PROGRESS: at sentence #10000, processed 205010 words and 101389 word types\n",
      "2019-02-11 14:47:41,848 : INFO : PROGRESS: at sentence #20000, processed 403566 words and 170975 word types\n",
      "2019-02-11 14:47:42,224 : INFO : PROGRESS: at sentence #30000, processed 599434 words and 234196 word types\n",
      "2019-02-11 14:47:42,603 : INFO : PROGRESS: at sentence #40000, processed 798141 words and 286447 word types\n",
      "2019-02-11 14:47:42,998 : INFO : PROGRESS: at sentence #50000, processed 998259 words and 337166 word types\n",
      "2019-02-11 14:47:43,246 : INFO : collected 365018 word types from a corpus of 1107953 words (unigram + bigrams) and 55651 sentences\n",
      "2019-02-11 14:47:43,246 : INFO : using 365018 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2019-02-11 14:47:43,247 : INFO : source_vocab length 365018\n",
      "2019-02-11 14:47:47,091 : INFO : Phraser built with 1137 phrasegrams\n",
      "2019-02-11 14:47:47,102 : INFO : collecting all words and their counts\n",
      "2019-02-11 14:47:47,102 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2019-02-11 14:47:48,116 : INFO : PROGRESS: at sentence #10000, processed 177937 words and 100651 word types\n",
      "2019-02-11 14:47:49,103 : INFO : PROGRESS: at sentence #20000, processed 350165 words and 171156 word types\n",
      "2019-02-11 14:47:50,016 : INFO : PROGRESS: at sentence #30000, processed 520278 words and 235011 word types\n",
      "2019-02-11 14:47:50,925 : INFO : PROGRESS: at sentence #40000, processed 692661 words and 288512 word types\n",
      "2019-02-11 14:47:51,891 : INFO : PROGRESS: at sentence #50000, processed 866443 words and 340596 word types\n",
      "2019-02-11 14:47:52,433 : INFO : collected 369042 word types from a corpus of 961298 words (unigram + bigrams) and 55651 sentences\n",
      "2019-02-11 14:47:52,434 : INFO : using 369042 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2019-02-11 14:47:52,435 : INFO : source_vocab length 369042\n",
      "2019-02-11 14:47:56,770 : INFO : Phraser built with 2603 phrasegrams\n",
      "2019-02-11 14:48:06,144 : INFO : collecting all words and their counts\n",
      "2019-02-11 14:48:06,148 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-02-11 14:48:06,192 : INFO : PROGRESS: at sentence #10000, processed 175783 words, keeping 14778 word types\n",
      "2019-02-11 14:48:06,237 : INFO : PROGRESS: at sentence #20000, processed 346055 words, keeping 21104 word types\n",
      "2019-02-11 14:48:06,287 : INFO : PROGRESS: at sentence #30000, processed 514436 words, keeping 26353 word types\n",
      "2019-02-11 14:48:06,335 : INFO : PROGRESS: at sentence #40000, processed 684888 words, keeping 29786 word types\n",
      "2019-02-11 14:48:06,384 : INFO : PROGRESS: at sentence #50000, processed 857013 words, keeping 32922 word types\n",
      "2019-02-11 14:48:06,410 : INFO : collected 34965 word types from a corpus of 950847 raw words and 55651 sentences\n",
      "2019-02-11 14:48:06,411 : INFO : Loading a fresh vocabulary\n",
      "2019-02-11 14:48:06,477 : INFO : effective_min_count=1 retains 34965 unique words (100% of original 34965, drops 0)\n",
      "2019-02-11 14:48:06,478 : INFO : effective_min_count=1 leaves 950847 word corpus (100% of original 950847, drops 0)\n",
      "2019-02-11 14:48:06,575 : INFO : deleting the raw counts dictionary of 34965 items\n",
      "2019-02-11 14:48:06,577 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2019-02-11 14:48:06,578 : INFO : downsampling leaves estimated 711826 word corpus (74.9% of prior 950847)\n",
      "2019-02-11 14:48:06,701 : INFO : estimated required memory for 34965 words and 200 dimensions: 73426500 bytes\n",
      "2019-02-11 14:48:06,702 : INFO : resetting layer weights\n",
      "2019-02-11 14:48:07,201 : INFO : training model with 20 workers on 34965 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-02-11 14:48:07,675 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-02-11 14:48:07,688 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-02-11 14:48:07,703 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-02-11 14:48:07,717 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-02-11 14:48:07,719 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-02-11 14:48:07,725 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-02-11 14:48:07,730 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-02-11 14:48:07,742 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-02-11 14:48:07,754 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-02-11 14:48:07,756 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-02-11 14:48:07,760 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 14:48:07,761 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 14:48:07,763 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 14:48:07,765 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 14:48:07,766 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 14:48:07,767 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 14:48:07,768 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 14:48:07,769 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 14:48:07,776 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 14:48:07,778 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 14:48:07,779 : INFO : EPOCH - 1 : training on 950847 raw words (711679 effective words) took 0.6s, 1260985 effective words/s\n",
      "2019-02-11 14:48:08,286 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-02-11 14:48:08,290 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-02-11 14:48:08,295 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-02-11 14:48:08,300 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-02-11 14:48:08,301 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-02-11 14:48:08,309 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-02-11 14:48:08,323 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-02-11 14:48:08,339 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-02-11 14:48:08,342 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-02-11 14:48:08,350 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-02-11 14:48:08,353 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 14:48:08,354 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 14:48:08,355 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 14:48:08,356 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 14:48:08,357 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 14:48:08,358 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 14:48:08,359 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 14:48:08,366 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 14:48:08,367 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 14:48:08,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 14:48:08,369 : INFO : EPOCH - 2 : training on 950847 raw words (711892 effective words) took 0.6s, 1237603 effective words/s\n",
      "2019-02-11 14:48:08,851 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-02-11 14:48:08,864 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-02-11 14:48:08,875 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-02-11 14:48:08,883 : INFO : worker thread finished; awaiting finish of 16 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-11 14:48:08,889 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-02-11 14:48:08,899 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-02-11 14:48:08,911 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-02-11 14:48:08,918 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-02-11 14:48:08,920 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-02-11 14:48:08,931 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-02-11 14:48:08,943 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 14:48:08,946 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 14:48:08,948 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 14:48:08,950 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 14:48:08,950 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 14:48:08,951 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 14:48:08,952 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 14:48:08,953 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 14:48:08,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 14:48:08,958 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 14:48:08,959 : INFO : EPOCH - 3 : training on 950847 raw words (711489 effective words) took 0.6s, 1236954 effective words/s\n",
      "2019-02-11 14:48:09,476 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-02-11 14:48:09,485 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-02-11 14:48:09,487 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-02-11 14:48:09,493 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-02-11 14:48:09,504 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-02-11 14:48:09,522 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-02-11 14:48:09,529 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-02-11 14:48:09,531 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-02-11 14:48:09,540 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-02-11 14:48:09,541 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-02-11 14:48:09,544 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 14:48:09,550 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 14:48:09,552 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 14:48:09,553 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 14:48:09,554 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 14:48:09,555 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 14:48:09,555 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 14:48:09,556 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 14:48:09,560 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 14:48:09,566 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 14:48:09,567 : INFO : EPOCH - 4 : training on 950847 raw words (711319 effective words) took 0.6s, 1202001 effective words/s\n",
      "2019-02-11 14:48:10,034 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-02-11 14:48:10,040 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-02-11 14:48:10,052 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-02-11 14:48:10,054 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-02-11 14:48:10,072 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-02-11 14:48:10,074 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-02-11 14:48:10,087 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-02-11 14:48:10,089 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-02-11 14:48:10,094 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-02-11 14:48:10,106 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-02-11 14:48:10,110 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 14:48:10,115 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 14:48:10,116 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 14:48:10,117 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 14:48:10,118 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 14:48:10,119 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 14:48:10,120 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 14:48:10,122 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 14:48:10,126 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 14:48:10,128 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 14:48:10,129 : INFO : EPOCH - 5 : training on 950847 raw words (711473 effective words) took 0.5s, 1300477 effective words/s\n",
      "2019-02-11 14:48:10,130 : INFO : training on a 4754235 raw words (3557852 effective words) took 2.9s, 1215044 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', 'shaken', 'as', 'we_are', ',', 'so', 'wan', 'with', 'care', 'Find']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "#documents = [\"the mayor of new york was there\", \"human computer interaction and machine learning has now become a trending research area\",\"human computer interaction is interesting\",\"human computer interaction is a pretty interesting subject\", \"human computer interaction is a great and new subject\", \"machine learning can be useful sometimes\",\"new york mayor was present\", \"I love machine learning because it is a new subject area\", \"human computer interaction helps people to get user friendly applications\"]\n",
    "\n",
    "sentence_stream = [doc.split(\" \") for doc in documents]\n",
    "\n",
    "trigram_sentences_project = []\n",
    "\n",
    "#bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')\n",
    "#trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')\n",
    "#bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ', threshold=2)\n",
    "#trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ') #, threshold=3\n",
    "\n",
    "bigram = Phraser(Phrases(sentence_stream))\n",
    "trigram = Phraser(Phrases(bigram[sentence_stream]))\n",
    "\n",
    "for sent in sentence_stream:\n",
    "    bigrams_ = bigram[sent]\n",
    "    trigrams_ = trigram[bigram[sent]]\n",
    "    trigram_sentences_project.append(trigrams_)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 1    # Minimum word count                        \n",
    "num_workers = 20      # Number of threads to run in parallel\n",
    "context = 5           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "model = word2vec.Word2Vec(trigram_sentences_project, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "vocab = list(model.wv.vocab.keys())\n",
    "print(vocab[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34965\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of items in our model's vocabulary\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-11 14:48:15,486 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('gentleman', 0.9454977512359619),\n",
       " ('maid', 0.9449328780174255),\n",
       " ('child', 0.931004524230957),\n",
       " ('fellow', 0.9295815229415894),\n",
       " ('fault', 0.9224992990493774),\n",
       " ('case', 0.9197366237640381),\n",
       " ('threadbare', 0.9118105173110962),\n",
       " ('young', 0.9111688137054443),\n",
       " ('dog', 0.9110386371612549),\n",
       " ('prince', 0.9039336442947388)]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"king\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.9479358196258545),\n",
       " ('master', 0.9403353929519653),\n",
       " ('lord', 0.934485673904419),\n",
       " ('fellow', 0.9323917627334595),\n",
       " ('brother', 0.9322241544723511),\n",
       " ('son', 0.9321994781494141),\n",
       " ('daughter', 0.9275787472724915),\n",
       " ('friend', 0.9243905544281006),\n",
       " ('wife', 0.923072099685669),\n",
       " ('husband', 0.9228538274765015)]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"lady\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life', 0.9239870309829712),\n",
       " ('honor', 0.9152224063873291),\n",
       " ('sorrow', 0.9139392375946045),\n",
       " ('beauty', 0.9132205247879028),\n",
       " ('fortune', 0.9071530699729919),\n",
       " ('virtue', 0.9049279689788818),\n",
       " ('grace', 0.8995459079742432),\n",
       " ('state', 0.8992392420768738),\n",
       " ('light', 0.8976709842681885),\n",
       " ('law', 0.8937926292419434)]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"death\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10029395571610536\n",
      "0.06176809774927151\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.relative_cosine_similarity(\"lady\",\"lord\",topn=10))\n",
    "print(model.wv.relative_cosine_similarity(\"lady\",\"meal\",topn=10))\n",
    "# From the gensim documentation: \"For WordNet synonyms, if rcs(topn=10) is greater than 0.10 \n",
    "# then wa and wb are more similar than any arbitrary word pairs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weeps', 0.9383993148803711),\n",
       " ('lively', 0.9343416094779968),\n",
       " ('purged', 0.933335542678833),\n",
       " ('plays', 0.9290236234664917),\n",
       " ('talks', 0.9243598580360413),\n",
       " ('stabbed', 0.9229390621185303),\n",
       " ('began', 0.9221399426460266),\n",
       " ('attaint', 0.9214203357696533),\n",
       " ('treads', 0.9203882217407227),\n",
       " ('sits', 0.9200345277786255)]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analogies -- this example asks:\n",
    "# \"she\" is to \"sings\" as \"he\" is to ...   (analogies are often written like this: \"she:sings::he:?\")\n",
    "model.wv.most_similar(positive=['she','sings'],negative=['he'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', 0.003717764),\n",
       " ('think', 0.003693466),\n",
       " ('thee', 0.0036249887),\n",
       " ('would', 0.0032487833),\n",
       " ('believe', 0.0031791634),\n",
       " ('If', 0.0031616038),\n",
       " ('know', 0.0030857706),\n",
       " ('can', 0.0021131453),\n",
       " ('might', 0.0020487302),\n",
       " ('cannot', 0.0016657811),\n",
       " ('love', 0.0016570459),\n",
       " ('if', 0.0015760583),\n",
       " ('myself', 0.0015052943),\n",
       " ('not', 0.0014749805),\n",
       " ('remember', 0.0014533957),\n",
       " ('confess', 0.0012912696),\n",
       " ('understand', 0.0012864495),\n",
       " ('wish', 0.0012587492),\n",
       " ('swear', 0.0012058887),\n",
       " ('intend', 0.0011748188),\n",
       " ('as', 0.0010575339),\n",
       " ('dare_not', 0.0010516727),\n",
       " ('should', 0.001024504),\n",
       " ('I_am_sorry', 0.0010206674),\n",
       " ('so_much', 0.0010177927),\n",
       " ('heard', 0.001006842),\n",
       " ('As', 0.0009884691),\n",
       " ('repent', 0.0009536792),\n",
       " ('dare', 0.00092651276),\n",
       " ('entreat', 0.0009140744)]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_output_word([\"I\",\"do\",\"love\"], topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
